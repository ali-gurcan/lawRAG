# ğŸ”§ Troubleshooting Guide

## âš ï¸ Segmentation Fault

### Sorun:
```
zsh: segmentation fault  python3 app_rag.py
```

### Nedenleri:
1. **RAM YetersizliÄŸi** - Llama 3.2-1B bile ~3-4GB gerektirir
2. **PyTorch/Transformers UyumsuzluÄŸu** - Versiyon Ã§akÄ±ÅŸmasÄ±
3. **Metal/MPS Sorunu** - macOS GPU backend hatasÄ±

---

## âœ… Ã‡Ã¶zÃ¼mler

### 1ï¸âƒ£ **Daha Hafif Model Kullan (En Kolay)**

`config.yaml` dosyasÄ±nÄ± dÃ¼zenle:

```yaml
active_models:
  llm: qwen-1.5b  # Llama yerine Qwen (token gerektirmez)
```

veya

```yaml
active_models:
  llm: gemma-2b   # Google Gemma (token gerektirmez)
```

**Avantaj:** Token gerekmez, daha az RAM

---

### 2ï¸âƒ£ **RAM Optimizasyonu**

DiÄŸer uygulamalarÄ± kapat:
```bash
# Bellek kullanÄ±mÄ±nÄ± kontrol et
top -o MEM
```

**16GB RAM iÃ§in Ã¶nerilen:**
- Chrome/Safari: Kapat
- Docker/VM: Durdur
- Gereksiz uygulamalar: Kapat

---

### 3ï¸âƒ£ **PyTorch/Transformers GÃ¼ncelle**

```bash
cd ~/Desktop/nlp
source venv/bin/activate

# GÃ¼ncel versiyonlarÄ± kur
pip install --upgrade torch transformers accelerate

# Veya belirli versiyonlar
pip install torch==2.1.0 transformers==4.35.2
```

---

### 4ï¸âƒ£ **Cache Temizle ve Yeniden BaÅŸlat**

```bash
cd ~/Desktop/nlp

# Cache'i temizle
rm -rf cache/*

# Tekrar dene
python3 app_rag.py
```

---

### 5ï¸âƒ£ **Sadece Retrieval Kullan (LLM'siz)**

EÄŸer hiÃ§biri Ã§alÄ±ÅŸmazsa, sadece retrieval kullan:

`config.yaml`:
```yaml
# LLM'i geÃ§ici olarak devre dÄ±ÅŸÄ± bÄ±rak
# Sadece kaynak gÃ¶sterimi yapacak
```

**NOT:** Bu Ã¶zellik henÃ¼z implement edilmedi, ama eklenebilir.

---

## ğŸ› Debug AdÄ±mlarÄ±

### 1. Model Ä°ndirmeyi Test Et

```bash
cd ~/Desktop/nlp
source venv/bin/activate

python3 -c "
from transformers import AutoModelForCausalLM
import os
os.environ['HF_TOKEN'] = 'your_token'

try:
    model = AutoModelForCausalLM.from_pretrained(
        'meta-llama/Llama-3.2-1B-Instruct',
        token=os.environ['HF_TOKEN'],
        low_cpu_mem_usage=True
    )
    print('âœ… Model loaded successfully!')
except Exception as e:
    print(f'âŒ Error: {e}')
"
```

### 2. RAM KullanÄ±mÄ±nÄ± Ä°zle

BaÅŸka bir terminal'de:
```bash
while true; do
    clear
    echo "RAM KullanÄ±mÄ±:"
    ps aux | grep python | head -5
    sleep 2
done
```

### 3. Verbose Logging

```bash
export TRANSFORMERS_VERBOSITY=debug
python3 app_rag.py
```

---

## ğŸ’¡ Ã–nerilen Ã‡Ã¶zÃ¼m (16GB RAM)

### SeÃ§enek A: Qwen Kullan (En Stabil)
```yaml
# config.yaml
active_models:
  embedding: e5-large
  llm: qwen-1.5b  # âœ… Token gerektirmez, stabil
```

**RAM:** ~7GB  
**Kalite:** 7/10  
**Stabilite:** YÃ¼ksek

---

### SeÃ§enek B: MPNet + Qwen (GÃ¼venli)
```yaml
# config.yaml
active_models:
  embedding: mpnet  # E5-Large yerine daha kÃ¼Ã§Ã¼k
  llm: qwen-1.5b
```

**RAM:** ~5.5GB  
**Kalite:** 6.5/10  
**Stabilite:** Ã‡ok YÃ¼ksek

---

### SeÃ§enek C: Llama ile Dene (Riskli)

EÄŸer Llama kullanmak zorundasÄ±n:

1. **TÃ¼m uygulamalarÄ± kapat**
2. **Cache temizle:** `rm -rf cache/*`
3. **Sistemi yeniden baÅŸlat**
4. **Sadece terminal aÃ§ ve Ã§alÄ±ÅŸtÄ±r**

```bash
cd ~/Desktop/nlp
source venv/bin/activate
python3 app_rag.py
```

**RAM:** ~9-10GB  
**Kalite:** 7.5/10  
**Stabilite:** Orta (segfault riski var)

---

## ğŸ” Hata MesajlarÄ±na GÃ¶re Ã‡Ã¶zÃ¼mler

### "Failed to resolve 'huggingface.co'"
**Ã‡Ã¶zÃ¼m:** Ä°nternet baÄŸlantÄ±sÄ±nÄ± kontrol et, VPN kapat

### "Access to model is restricted"
**Ã‡Ã¶zÃ¼m:** HF_TOKEN ekle ve model eriÅŸimi al

### "Out of memory"
**Ã‡Ã¶zÃ¼m:** Daha kÃ¼Ã§Ã¼k model kullan (qwen-1.5b)

### "Segmentation fault"
**Ã‡Ã¶zÃ¼m:**
1. PyTorch/Transformers gÃ¼ncelle
2. Daha kÃ¼Ã§Ã¼k model kullan
3. Cache temizle
4. Sistemi yeniden baÅŸlat

---

## ğŸ“ HÄ±zlÄ± YardÄ±m

### Sorun Devam Ediyorsa:

1. **config.yaml'Ä± ÅŸu ÅŸekilde deÄŸiÅŸtir:**
```yaml
active_models:
  embedding: mpnet
  llm: qwen-1.5b
```

2. **Cache'i temizle:**
```bash
rm -rf cache/*
```

3. **Tekrar Ã§alÄ±ÅŸtÄ±r:**
```bash
python3 app_rag.py
```

**Bu %95 Ã§alÄ±ÅŸÄ±r!** âœ…

---

## ğŸ¯ SonuÃ§

**16GB RAM iÃ§in BEST PRACTICE:**

```yaml
# config.yaml - STABIL SETUP
active_models:
  embedding: e5-large  # veya mpnet (daha gÃ¼venli)
  llm: qwen-1.5b       # Token gerektirmez, stabil

retrieval:
  top_k: 3
  use_hybrid_search: true
  use_reranking: true
  use_query_expansion: true
```

**Bu setup:**
- âœ… Token gerektirmez
- âœ… Segfault riski yok
- âœ… 7GB RAM kullanÄ±r
- âœ… Ä°yi kalite (7/10)
- âœ… Stabil Ã§alÄ±ÅŸÄ±r

**Llama istiyorsan:**
- TÃ¼m uygulamalarÄ± kapat
- 10GB+ boÅŸ RAM olmalÄ±
- Segfault riski var
- Qwen'e gÃ¶re sadece +0.5 puan daha iyi

**Karar:** Qwen kullan! ğŸ¯

